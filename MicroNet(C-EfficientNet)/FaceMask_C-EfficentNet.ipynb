{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import timm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '.../FaceMask_presence/ds'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model building blocks\n",
    "class HSwish(nn.Module):\n",
    "    \"\"\"\n",
    "    H-Swish activation function from 'Searching for MobileNetV3,' https://arxiv.org/abs/1905.02244.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    inplace : bool\n",
    "        Whether to use inplace version of the module.\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace=True):\n",
    "        super(HSwish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "        self.relu = nn.ReLU6(inplace = self.inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.relu(x + 3.0) / 6.0\n",
    "\n",
    "class MicroBlock(nn.Module):\n",
    "    '''expand + depthwise + pointwise\n",
    "    Activation : ReLU or HSwish\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes, expansion, stride, device, add_se = False, Activation = 'ReLU'):\n",
    "        super(MicroBlock, self).__init__()\n",
    "        self.out_planes = out_planes\n",
    "        self.stride = stride\n",
    "        planes = int(expansion * in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=0.01)\n",
    "        if self.stride ==1:\n",
    "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
    "        if self.stride ==2:\n",
    "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=5, stride=stride, padding=2, groups=planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=0.01)\n",
    "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes, momentum=0.01)\n",
    "        self.add_se = add_se\n",
    "        \n",
    "        if Activation == 'HSwish':\n",
    "            self.act1 = HSwish()\n",
    "            self.act2 = HSwish()\n",
    "            if self.add_se:\n",
    "                self.act_se = HSwish()\n",
    "        else:\n",
    "            self.act1 = nn.ReLU()\n",
    "            self.act2 = nn.ReLU()\n",
    "            if self.add_se:\n",
    "                self.act_se = nn.ReLU()\n",
    "            \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_planes, momentum=0.01)\n",
    "            )\n",
    "        \n",
    "        # SE layers\n",
    "        \n",
    "        if self.add_se:\n",
    "            self.avg_se = nn.AdaptiveAvgPool2d(1)\n",
    "            number = int(out_planes*0.25)\n",
    "            self.fc1 = nn.Conv2d(out_planes, number, kernel_size=1, bias=False)\n",
    "            self.fc2 = nn.Conv2d(number, out_planes, kernel_size=1, bias=False)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act1(self.bn1(self.conv1(x)))\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(self.bn2(out))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        # Squeeze-Excitation\n",
    "        if self.add_se:\n",
    "            w = self.avg_se(out)\n",
    "            w = self.act_se(self.fc1(w))\n",
    "            w = self.sigmoid(self.fc2(w))\n",
    "            \n",
    "            out = out * w + self.shortcut(x) if self.stride==1 else out\n",
    "            return out\n",
    "        \n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out\n",
    "\n",
    "\n",
    "class MicroNet_imagenet(nn.Module):\n",
    "    # (expansion, out_planes, num_blocks, stride)\n",
    "    def __init__(self, num_classes=1000, wide_factor = 1, depth_factor =1, add_se = True, Activation = 'HSwish'):\n",
    "        super(MicroNet_imagenet, self).__init__()\n",
    "        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n",
    "        '''\n",
    "        wide_factor: ratio to expand channel\n",
    "        depth_factor: ratio to expand depth\n",
    "        '''\n",
    "        self.cfg = [[1, 16, 2, 1],\n",
    "                    [3, 24, 1, 2],\n",
    "                    [3, 24, 2, 1],\n",
    "                    [3, 40, 1, 2],\n",
    "                    [3, 40, 2, 1],\n",
    "                    [3, 80, 1, 2],\n",
    "                    [3, 80, 2, 1],\n",
    "                    [3, 96, 2, 1],\n",
    "                    [3, 192, 1, 2],\n",
    "                    [3, 192, 3, 1],\n",
    "                    [3, 320, 1, 1]]\n",
    "\n",
    "        #reconstruct structure config\n",
    "        self.change_cfg(wide_factor, depth_factor)\n",
    "        #make train recipe\n",
    "        self.set_config(batch_size = 128, momentum = 0.9, lr = 0.1, num_epochs =200, criterion = nn.CrossEntropyLoss(), weight_decay = 1e-5, gamma = 0.1, milestones = [100, 150], device = 'cuda:0' if cuda.is_available() else 'cpu', nesterov = True)\n",
    "        \n",
    "        #construct network\n",
    "        self.add_se = add_se\n",
    "        self.Activation = Activation\n",
    "        self.input_channel = 32\n",
    "        self.last_channel = 640\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3, self.input_channel, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.input_channel, momentum=0.01)\n",
    "        self.layers = self._make_layers(in_planes=self.input_channel)\n",
    "        \n",
    "        self.last_conv = nn.Conv2d(self.cfg[-1][1], self.last_channel, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(self.last_channel, self.num_classes)\n",
    "        \n",
    "        if self.Activation == 'HSwish':\n",
    "            self.stem_act = HSwish()\n",
    "        else:\n",
    "            self.stem_act = nn.ReLU()\n",
    "        \n",
    "        #initialize the parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        #initialize the parameters\n",
    "        self.reset_custom_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def reset_custom_parameters(self):\n",
    "        for name, module in self.named_children():\n",
    "            if 'layer' in name:\n",
    "                if 'conv1' in name:\n",
    "                    n = module.shape[0]\n",
    "                    module.data.normal_(0, 0.73 * math.sqrt(2. / n))\n",
    "                elif 'conv2' in name:\n",
    "                    n = module.shape[0]\n",
    "                    module.data.normal_(0, 9.37 * math.sqrt(2. / n))\n",
    "                elif 'conv3' in name:\n",
    "                    n = module.shape[0]\n",
    "                    module.data.normal_(0, 0.55 * math.sqrt(2. / n))\n",
    "                    \n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1]*(num_blocks-1)\n",
    "            for stride in strides:\n",
    "                layers.append(MicroBlock(in_planes, out_planes, expansion, stride, self.device, self.add_se, self.Activation))\n",
    "                in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def change_cfg(self, wide_factor, depth_factor):\n",
    "        for i in range(len(self.cfg)):\n",
    "            self.cfg[i][1] = int(self.cfg[i][1] * wide_factor)\n",
    "            if self.cfg[i][3] ==1:\n",
    "                self.cfg[i][2] = int(self.cfg[i][2] * depth_factor)\n",
    "    \n",
    "    \n",
    "    def set_config(self, batch_size, momentum, lr, num_epochs, device, weight_decay, gamma = 0.1, milestones = [100,150], nesterov = True, criterion = nn.CrossEntropyLoss()):\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = criterion\n",
    "        self.weight_decay = weight_decay\n",
    "        self.gamma = gamma\n",
    "        self.milestones = milestones\n",
    "        self.device = device\n",
    "        self.nesterov = nesterov\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.stem_act(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = self.last_conv(out)\n",
    "        out = self.avg(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(self.dropout(out))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model initialization\n",
    "model_ft = MicroNet_imagenet(num_classes = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.3417 Acc: 0.8491\n",
      "val Loss: 0.2085 Acc: 0.9271\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.2099 Acc: 0.9125\n",
      "val Loss: 0.1714 Acc: 0.9417\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.1813 Acc: 0.9248\n",
      "val Loss: 0.1133 Acc: 0.9610\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.1626 Acc: 0.9325\n",
      "val Loss: 0.1071 Acc: 0.9617\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.1541 Acc: 0.9337\n",
      "val Loss: 0.1113 Acc: 0.9577\n",
      "\n",
      "Training complete in 13m 52s\n",
      "Best val Acc: 0.961749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-EfficentNet_net 18.967240352630615  224_224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.967240352630615"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference time estiamtion\n",
    "from utils import get_time_my\n",
    "get_time_dima('C-EfficentNet_net',model_ft,224,224,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model conversion to onnx format\n",
    "dummy = torch.randn(10, 3, 224, 224, device='cuda')\n",
    "input_names = [ \"actual_input_1\" ] \n",
    "output_names = [ \"output1\" ]\n",
    "torch.onnx.export(model_ft, dummy, \"FaceMask_C-EfficientNet_x1.1_224.onnx\", verbose=False, input_names=input_names, output_names=output_names,export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
